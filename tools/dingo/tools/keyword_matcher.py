"""
Keyword Matcher Tool for Dingo - ATS-Optimized Resume-JD Matching

Implements industry-standard TF-IDF weighted keyword matching algorithm used by 98% of Fortune 500 ATS systems.
Combines Resume-Matcher's frequency-based priority classification with LLM-powered optimization recommendations.

Algorithm:
1. Dual-Engine Extraction: Extract keywords from both resume and JD using keyword_extraction logic
2. TF-IDF Weighting: Calculate keyword importance based on frequency in JD
3. Priority Classification: High (â‰¥3 mentions), Medium (2 mentions), Low (1 mention)
4. Weighted Scoring: Calculate match score with priority-based weights
5. LLM Recommendations: Generate actionable optimization suggestions

Reference: 
- Resume-Matcher/apps/backend/app/services/score_improvement_service.py
- TF-IDF algorithm used by 98% Fortune 500 companies (LinkedIn, 2021)
"""

import re
import json
import time
from pathlib import Path
from typing import Any
from collections.abc import Generator

from dify_plugin import Tool
from dify_plugin.entities.tool import ToolInvokeMessage
from dify_plugin.entities.model.llm import LLMModelConfig
from dify_plugin.entities.model.message import UserPromptMessage


class KeywordMatcher(Tool):
    """
    ATS-Optimized Keyword Matcher: TF-IDF Weighted Matching + LLM Recommendations
    
    Implements the same algorithm used by major ATS systems (Taleo, Workday, Greenhouse)
    to calculate resume-job description match scores.
    """
    
    # Keywords that need case-sensitive matching
    CASE_SENSITIVE_KEYWORDS = {"Go", "R"}
    
    # Synonym mapping (same as keyword_extraction)
    SYNONYM_MAP = {
        "k8s": "Kubernetes",
        "js": "JavaScript",
        "ts": "TypeScript",
        "py": "Python",
        "tf": "TensorFlow",
        "react.js": "React",
        "vue.js": "Vue.js",
        "node.js": "Node.js",
        "next.js": "Next.js",
        "express.js": "Express.js",
        "nest.js": "NestJS",
        "postgresql": "PostgreSQL",
        "mysql": "MySQL",
        "mongodb": "MongoDB",
        "aws": "AWS",
        "gcp": "GCP",
        "ci/cd": "CI/CD",
        "ml": "Machine Learning",
        "ai": "Artificial Intelligence",
        "nlp": "Natural Language Processing",
        "cv": "Computer Vision",
    }
    
    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:
        try:
            resume_text = tool_parameters.get('resume_text', '').strip()
            resume_keywords_json = tool_parameters.get('resume_keywords', '').strip()
            jd_text = tool_parameters.get('jd_text', '').strip()
            position_name = tool_parameters.get('position_name', '').strip()
            use_llm = tool_parameters.get('use_llm', True)

            if not resume_text:
                yield self.create_text_message("âŒ Resume text cannot be empty")
                return

            # Must provide either jd_text or position_name
            if not jd_text and not position_name:
                yield self.create_text_message("âŒ å¿…é¡»æä¾› jd_textï¼ˆå®Œæ•´èŒä½æè¿°ï¼‰æˆ– position_nameï¼ˆèŒä½åç§°ï¼‰ä¹‹ä¸€")
                return

            # Load keyword dictionary
            current_dir = Path(__file__).parent.parent
            dictionary_path = current_dir / "data" / "onet_keywords.json"
            keywords = self._load_dictionary(dictionary_path)

            # 1. Get resume keywords (reuse if provided, otherwise extract)
            if resume_keywords_json:
                # Try to parse the input intelligently
                resume_keywords = self._parse_resume_keywords_input(resume_keywords_json)

                if resume_keywords is None:
                    # Parsing failed, extract from resume text instead
                    resume_keywords = self._extract_keywords_dual_engine(resume_text, use_llm, keywords)
            else:
                # Extract keywords from resume
                resume_keywords = self._extract_keywords_dual_engine(resume_text, use_llm, keywords)

            # 2. Get JD keywords: either from provided JD text or generate from position name
            if jd_text:
                # User provided full JD text
                jd_keywords = self._extract_keywords_dual_engine(jd_text, use_llm, keywords)
                jd_source = "ç”¨æˆ·æä¾›çš„èŒä½æè¿°"
            else:
                # User only provided position name, use LLM to generate standard requirements
                if not use_llm:
                    yield self.create_text_message("âŒ ä½¿ç”¨èŒä½åç§°ç”Ÿæˆæ ‡å‡†è¦æ±‚æ—¶ï¼Œå¿…é¡»å¯ç”¨ LLMï¼ˆuse_llm=trueï¼‰")
                    return

                generated_jd = self._generate_standard_jd_requirements(position_name)
                jd_keywords = self._extract_keywords_from_generated_jd(generated_jd)
                jd_source = f"LLM ç”Ÿæˆçš„æ ‡å‡†èŒä½è¦æ±‚ï¼ˆ{position_name}ï¼‰"
                # Use generated JD as jd_text for display
                jd_text = generated_jd

            # 3. Perform matching analysis
            match_result = self._calculate_match_score(
                resume_keywords, jd_keywords, resume_text, jd_text, use_llm, jd_source
            )

            # Create summary text
            summary = self._create_summary(match_result, True)

            # Yield results
            json_message = self.create_json_message(match_result)
            text_message = self.create_text_message(summary)
            yield from [json_message, text_message]

        except Exception as e:
            yield self.create_text_message(f"âŒ Keyword matching failed: {str(e)}")
    
    def _load_dictionary(self, dictionary_path: Path) -> list[str]:
        """Load O*NET keyword dictionary"""
        with open(dictionary_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        all_keywords = []
        for category_keywords in data['keywords'].values():
            all_keywords.extend(category_keywords)
        
        return all_keywords
    
    def _normalize_synonyms(self, text: str) -> str:
        """Normalize synonyms (K8sâ†’Kubernetes, etc.)"""
        normalized = text
        for synonym, standard in self.SYNONYM_MAP.items():
            pattern = re.compile(rf'\b{re.escape(synonym)}\b', re.IGNORECASE)
            normalized = pattern.sub(standard, normalized)
        return normalized
    
    def _prepare_text_for_matching(self, text: str) -> str:
        """
        Prepare text for keyword matching (Resume-Matcher pattern)
        Remove markdown symbols but preserve technical terms like C#, C++
        """
        lowered = text.lower()
        lowered = re.sub(r"[`*_>\-]", " ", lowered)
        lowered = re.sub(r"(?<![a-z])#(?![a-z])", " ", lowered)
        lowered = re.sub(r"\s+", " ", lowered)
        return lowered
    
    def _count_mentions(self, keyword: str, text: str) -> int:
        """Count keyword mentions in text (case-sensitive for special keywords)"""
        if keyword in self.CASE_SENSITIVE_KEYWORDS:
            pattern = re.compile(rf"(?<!\w){re.escape(keyword)}(?!\w)")
            return len(pattern.findall(text))
        else:
            text_normalized = self._prepare_text_for_matching(text)
            kw_lower = keyword.lower()
            pattern = re.compile(rf"(?<!\w){re.escape(kw_lower)}(?!\w)")
            return len(pattern.findall(text_normalized))

    def _extract_with_dictionary(self, text: str, keywords: list[str]) -> list[dict[str, Any]]:
        """Extract keywords using dictionary matching (Engine 1)"""
        text_normalized = self._normalize_synonyms(text)
        text_norm = self._prepare_text_for_matching(text_normalized)

        results = []
        for keyword in keywords:
            if keyword in self.CASE_SENSITIVE_KEYWORDS:
                pattern = re.compile(rf"(?<!\w){re.escape(keyword)}(?!\w)")
                mentions = len(pattern.findall(text_normalized))
            else:
                kw_lower = keyword.lower()
                pattern = re.compile(rf"(?<!\w){re.escape(kw_lower)}(?!\w)")
                mentions = len(pattern.findall(text_norm))

            if mentions > 0:
                results.append({
                    "skill": keyword,
                    "mentions": mentions,
                    "confidence": 1.0,
                    "source": "dictionary"
                })

        return results

    def _extract_with_llm(self, text: str) -> list[dict[str, Any]]:
        """Extract keywords using LLM semantic analysis (Engine 2)"""
        prompt = f"""You are a technical keyword extraction expert. Extract ALL technology keywords from this text.

Output ONLY valid JSON (no markdown, no code blocks):
{{
  "keywords": [
    {{"skill": "Python", "confidence": 1.0, "source": "explicit"}},
    {{"skill": "Docker", "confidence": 0.85, "source": "inferred"}}
  ]
}}

Text:
{text}"""

        llm_config = {
            "provider": "deepseek",
            "model": "deepseek-chat",
            "mode": "chat",
            "completion_params": {
                "temperature": 0.3,
                "max_tokens": 2000
            }
        }

        # Retry logic for LLM invocation
        max_retries = 3
        retry_delay = 1  # Initial delay in seconds

        for attempt in range(max_retries):
            try:
                llm_result = self.session.model.llm.invoke(
                    model_config=LLMModelConfig(**llm_config),
                    prompt_messages=[UserPromptMessage(content=prompt)],
                    stream=False
                )

                response_text = llm_result.message.content.strip()

                # Check for empty response
                if not response_text:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ LLM returned empty response (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                    else:
                        print(f"âŒ LLM returned empty response after {max_retries} attempts")
                        return []

                # Clean markdown code blocks
                response_text = re.sub(r'^```json\s*', '', response_text)
                response_text = re.sub(r'\s*```$', '', response_text)

                llm_data = json.loads(response_text)
                keywords = llm_data.get('keywords', [])

                if keywords:
                    return keywords
                else:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ LLM returned empty keywords list (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                    else:
                        return []

            except json.JSONDecodeError as json_err:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ JSON parsing failed (attempt {attempt + 1}/{max_retries}): {str(json_err)}, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    print(f"âŒ JSON parsing failed after {max_retries} attempts: {str(json_err)}")
                    return []

            except Exception as llm_err:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ LLM invocation failed (attempt {attempt + 1}/{max_retries}): {str(llm_err)}, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    print(f"âŒ LLM invocation failed after {max_retries} attempts: {str(llm_err)}")
                    return []

        return []

    def _merge_keywords(self, dict_results: list[dict], llm_results: list[dict]) -> list[dict]:
        """Merge and deduplicate keywords from both engines"""
        merged = {}

        for kw in dict_results:
            skill = kw['skill']
            merged[skill] = kw

        for kw in llm_results:
            skill = kw['skill']
            if skill not in merged:
                merged[skill] = kw
            else:
                merged[skill]['confidence'] = max(merged[skill]['confidence'], kw.get('confidence', 0.7))

        return list(merged.values())

    def _extract_keywords_dual_engine(self, text: str, use_llm: bool, keywords: list[str]) -> list[dict]:
        """Extract keywords using dual-engine architecture"""
        dict_results = self._extract_with_dictionary(text, keywords)

        if use_llm:
            llm_results = self._extract_with_llm(text)
            return self._merge_keywords(dict_results, llm_results)
        else:
            return dict_results

    def _build_skill_comparison(self, resume_keywords: list[dict], jd_keywords: list[dict],
                                resume_text: str, jd_text: str) -> list[dict]:
        """
        Build skill comparison statistics (Resume-Matcher algorithm)

        For each JD keyword, count mentions in both resume and JD to calculate:
        - Priority (based on JD frequency)
        - Weight (TF-IDF inspired)
        - Match status
        """
        jd_skills = {kw['skill'] for kw in jd_keywords}
        resume_skills = {kw['skill'] for kw in resume_keywords}

        stats = []
        for jd_kw in jd_keywords:
            skill = jd_kw['skill']

            # Count mentions in both texts
            jd_mentions = self._count_mentions(skill, jd_text)
            resume_mentions = self._count_mentions(skill, resume_text)

            # Priority classification (Resume-Matcher pattern)
            if jd_mentions >= 3:
                priority = "high"
                weight = 3.0
            elif jd_mentions == 2:
                priority = "medium"
                weight = 2.0
            else:
                priority = "low"
                weight = 1.0

            stats.append({
                "skill": skill,
                "resume_mentions": resume_mentions,
                "jd_mentions": jd_mentions,
                "priority": priority,
                "weight": weight,
                "matched": resume_mentions > 0
            })

        return stats

    def _calculate_match_score(self, resume_keywords: list[dict], jd_keywords: list[dict],
                               resume_text: str, jd_text: str, use_llm: bool, jd_source: str = "ç”¨æˆ·æä¾›çš„èŒä½æè¿°") -> dict:
        """
        Calculate ATS match score using TF-IDF weighted algorithm

        Args:
            resume_keywords: Extracted resume keywords
            jd_keywords: Extracted JD keywords
            resume_text: Original resume text
            jd_text: Original JD text
            use_llm: Whether to use LLM for recommendations
            jd_source: Source of JD keywords (for display purposes)

        Returns comprehensive match analysis with:
        - Weighted match score (priority-based)
        - Simple match score (for comparison)
        - Matched/missing keywords breakdown
        - LLM-generated recommendations
        """
        # Build skill comparison statistics
        stats = self._build_skill_comparison(resume_keywords, jd_keywords, resume_text, jd_text)

        # Calculate weighted match score
        total_weight = sum(s['weight'] for s in stats)
        matched_weight = sum(s['weight'] for s in stats if s['matched'])
        weighted_score = round((matched_weight / total_weight * 100) if total_weight > 0 else 0, 1)

        # Calculate simple match score (for comparison)
        total_keywords = len(stats)
        matched_keywords = sum(1 for s in stats if s['matched'])
        simple_score = round((matched_keywords / total_keywords * 100) if total_keywords > 0 else 0, 1)

        # Categorize keywords
        matched = [s for s in stats if s['matched']]
        missing = [s for s in stats if not s['matched']]

        # Sort by priority
        matched_high = [s for s in matched if s['priority'] == 'high']
        matched_medium = [s for s in matched if s['priority'] == 'medium']
        matched_low = [s for s in matched if s['priority'] == 'low']

        missing_high = [s for s in missing if s['priority'] == 'high']
        missing_medium = [s for s in missing if s['priority'] == 'medium']
        missing_low = [s for s in missing if s['priority'] == 'low']

        # Generate LLM recommendations
        if use_llm and missing:
            recommendations = self._generate_recommendations(
                resume_text, jd_text, matched, missing,
                missing_high, missing_medium, weighted_score
            )
        else:
            recommendations = self._generate_rule_based_recommendations(
                missing_high, missing_medium, weighted_score
            )

        return {
            "match_analysis": {
                "weighted_match_score": weighted_score,
                "simple_match_score": simple_score,
                "total_resume_keywords": len(resume_keywords),
                "total_jd_keywords": len(jd_keywords),
                "matched_count": matched_keywords,
                "missing_count": len(missing)
            },
            "keywords": {
                "matched": {
                    "high_priority": [{"skill": s['skill'], "mentions": s['resume_mentions']} for s in matched_high],
                    "medium_priority": [{"skill": s['skill'], "mentions": s['resume_mentions']} for s in matched_medium],
                    "low_priority": [{"skill": s['skill'], "mentions": s['resume_mentions']} for s in matched_low]
                },
                "missing": {
                    "high_priority": [{"skill": s['skill'], "jd_mentions": s['jd_mentions']} for s in missing_high],
                    "medium_priority": [{"skill": s['skill'], "jd_mentions": s['jd_mentions']} for s in missing_medium],
                    "low_priority": [{"skill": s['skill'], "jd_mentions": s['jd_mentions']} for s in missing_low]
                }
            },
            "recommendations": recommendations
        }

    def _generate_recommendations(self, resume_text: str, jd_text: str,
                                  matched: list[dict], missing: list[dict],
                                  missing_high: list[dict], missing_medium: list[dict],
                                  weighted_score: float) -> str:
        """Generate LLM-powered optimization recommendations"""

        matched_skills = ", ".join([s['skill'] for s in matched[:15]])
        missing_high_skills = ", ".join([s['skill'] for s in missing_high])
        missing_medium_skills = ", ".join([s['skill'] for s in missing_medium])

        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç®€å†ä¼˜åŒ–ä¸“å®¶å’Œ ATS ç³»ç»Ÿä¸“å®¶ã€‚åŸºäºå…³é”®è¯åŒ¹é…åˆ†æï¼Œç›´æ¥ç»™å‡ºå…·ä½“çš„ç®€å†ä¼˜åŒ–å»ºè®®ã€‚

## åŒ¹é…åˆ†æç»“æœ
- **ATS åŒ¹é…åº¦**: {weighted_score}%
- **å·²åŒ¹é…å…³é”®è¯**: {matched_skills}
- **ç¼ºå¤±å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰**: {missing_high_skills or "æ— "}
- **ç¼ºå¤±å…³é”®è¯ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰**: {missing_medium_skills or "æ— "}

## ç®€å†å†…å®¹
{resume_text[:2000]}

## èŒä½æè¿°
{jd_text[:2000]}

## è¾“å‡ºè¦æ±‚

**ä¸è¦**è‡ªæˆ‘ä»‹ç»ã€ä¸è¦åˆ†æé—®é¢˜ã€ä¸è¦ä»‹ç»å·¥ä½œè®¡åˆ’ï¼Œ**ç›´æ¥å¼€å§‹è¾“å‡ºä¼˜åŒ–å»ºè®®**ã€‚

æ¯æ¡å»ºè®®å¿…é¡»åŒ…å«ï¼š
- **æ”¹å‰**ï¼šä»ç®€å†ä¸­æ‘˜å½•éœ€è¦ä¿®æ”¹çš„åŸæ–‡ï¼ˆå¦‚æœæ˜¯æ–°å¢å†…å®¹ï¼Œå†™"æ— "ï¼‰
- **æ”¹å**ï¼šä¼˜åŒ–åçš„è¡¨è¿°ï¼ˆå¯ç›´æ¥å¤åˆ¶ç²˜è´´ä½¿ç”¨ï¼‰
- **ä¼˜åŒ–ç†ç”±**ï¼š1-2 å¥è¯è¯´æ˜ä¸ºä»€ä¹ˆè¿™æ ·æ”¹ï¼Œé‡ç‚¹è¯´æ˜å¦‚ä½•æå‡ ATS åŒ¹é…åº¦

**é‡è¦**ï¼šå¦‚æœæŸä¸ªä¼˜å…ˆçº§æ²¡æœ‰ä¼˜åŒ–å»ºè®®ï¼ˆä¾‹å¦‚ç¼ºå¤±å…³é”®è¯ä¸º"æ— "æˆ–ç®€å†å·²ç»å¾ˆå¥½ï¼‰ï¼Œ**ç›´æ¥è·³è¿‡è¯¥éƒ¨åˆ†**ï¼Œä¸è¦è¾“å‡º"æ”¹å‰ï¼šï¼ˆæ— ï¼‰æ”¹åï¼šï¼ˆæ— ï¼‰"è¿™æ ·çš„ç©ºå†…å®¹ã€‚

## è¾“å‡ºæ ¼å¼

### ğŸ”´ é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆå¿…é¡»è¡¥å……ï¼‰

**ä»…åœ¨æœ‰ç¼ºå¤±çš„é«˜ä¼˜å…ˆçº§å…³é”®è¯æ—¶è¾“å‡ºæ­¤éƒ¨åˆ†**

**æ”¹å‰**ï¼š
```
[ä»ç®€å†ä¸­æ‘˜å½•çš„åŸæ–‡ï¼Œå¦‚æœæ˜¯æ–°å¢å†…å®¹åˆ™å†™"æ— "]
```

**æ”¹å**ï¼š
```
[ä¼˜åŒ–åçš„è¡¨è¿°ï¼ŒåŒ…å«ç¼ºå¤±çš„é«˜ä¼˜å…ˆçº§å…³é”®è¯]
```

**ä¼˜åŒ–ç†ç”±**ï¼š[è¯´æ˜å¦‚ä½•æå‡ ATS åŒ¹é…åº¦]

---

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ä¼˜åŒ–ï¼ˆå»ºè®®è¡¥å……ï¼‰

**ä»…åœ¨æœ‰ç¼ºå¤±çš„ä¸­ä¼˜å…ˆçº§å…³é”®è¯æ—¶è¾“å‡ºæ­¤éƒ¨åˆ†**

**æ”¹å‰**ï¼š
```
[åŸæ–‡æˆ–"æ— "]
```

**æ”¹å**ï¼š
```
[ä¼˜åŒ–åçš„è¡¨è¿°ï¼ŒåŒ…å«ç¼ºå¤±çš„ä¸­ä¼˜å…ˆçº§å…³é”®è¯]
```

**ä¼˜åŒ–ç†ç”±**ï¼š[è¯´æ˜å¦‚ä½•æå‡ ATS åŒ¹é…åº¦]

---

### ğŸŸ¢ å·²åŒ¹é…å…³é”®è¯ä¼˜åŒ–ï¼ˆå¼ºåŒ–è¡¨è¿°ï¼‰

**ä»…åœ¨å·²åŒ¹é…å…³é”®è¯å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ—¶è¾“å‡ºæ­¤éƒ¨åˆ†**

**æ”¹å‰**ï¼š
```
[åŸæ–‡]
```

**æ”¹å**ï¼š
```
[ä¼˜åŒ–åçš„è¡¨è¿°ï¼Œå¢åŠ å…³é”®è¯å¯†åº¦æˆ–é‡åŒ–æŒ‡æ ‡]
```

**ä¼˜åŒ–ç†ç”±**ï¼š[è¯´æ˜å¦‚ä½•æ›´å¥½åœ°çªå‡ºå·²åŒ¹é…å…³é”®è¯]

---

## ä¼˜åŒ–é‡ç‚¹

1. **è¡¥å……ç¼ºå¤±å…³é”®è¯**ï¼šä¼˜å…ˆè¡¥å……é«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆ{missing_high_skills or "æ— "}ï¼‰
2. **å¢åŠ å…³é”®è¯å¯†åº¦**ï¼šå·²åŒ¹é…å…³é”®è¯è¦åœ¨ç®€å†ä¸­å‡ºç° 2-3 æ¬¡
3. **é‡åŒ–æˆæœ**ï¼šç”¨æ•°æ®è¯´è¯ï¼ˆå¦‚ï¼šæ€§èƒ½æå‡ X%ã€å¤„ç†é‡ X ä¸‡æ¬¡/æ—¥ï¼‰
4. **ATS å‹å¥½æ ¼å¼**ï¼šé¿å…è¡¨æ ¼ã€å›¾ç‰‡ã€ç‰¹æ®Šç¬¦å·ï¼Œä½¿ç”¨æ ‡å‡†å­—ä½“å’Œæ ‡é¢˜
5. **è‡ªç„¶èå…¥**ï¼šå…³é”®è¯è¦è‡ªç„¶èå…¥å¥å­ï¼Œä¸è¦ç”Ÿç¡¬å †ç Œ

---

**ç°åœ¨å¼€å§‹è¾“å‡ºä¼˜åŒ–å»ºè®®**ï¼ˆä¸è¦ä»»ä½•å¼€åœºç™½ï¼Œç›´æ¥ä»ç¬¬ä¸€æ¡å»ºè®®å¼€å§‹ï¼‰ï¼š"""

        llm_config = {
            "provider": "deepseek",
            "model": "deepseek-chat",
            "mode": "chat",
            "completion_params": {
                "temperature": 0.7,
                "max_tokens": 3000
            }
        }

        # Retry logic for LLM invocation
        max_retries = 3
        retry_delay = 1  # Initial delay in seconds

        for attempt in range(max_retries):
            try:
                llm_result = self.session.model.llm.invoke(
                    model_config=LLMModelConfig(**llm_config),
                    prompt_messages=[UserPromptMessage(content=prompt)],
                    stream=False
                )

                response_text = llm_result.message.content.strip()

                # Check for empty response
                if not response_text:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ LLM returned empty recommendations (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                    else:
                        print(f"âŒ LLM returned empty recommendations after {max_retries} attempts, using fallback")
                        return self._generate_rule_based_recommendations(missing_high, missing_medium, weighted_score)

                return response_text

            except Exception as llm_err:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ LLM recommendation generation failed (attempt {attempt + 1}/{max_retries}): {str(llm_err)}, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    print(f"âŒ LLM recommendation generation failed after {max_retries} attempts, using fallback")
                    return self._generate_rule_based_recommendations(missing_high, missing_medium, weighted_score)

        # Fallback to rule-based recommendations
        return self._generate_rule_based_recommendations(missing_high, missing_medium, weighted_score)

    def _generate_rule_based_recommendations(self, missing_high: list[dict],
                                            missing_medium: list[dict],
                                            weighted_score: float) -> str:
        """Generate rule-based recommendations (when LLM is disabled)"""
        recommendations = []

        recommendations.append(f"## ATS åŒ¹é…åº¦: {weighted_score}%\n")

        if weighted_score >= 80:
            recommendations.append("âœ… **ä¼˜ç§€**ï¼šæ‚¨çš„ç®€å†ä¸èŒä½æè¿°é«˜åº¦åŒ¹é…ï¼")
        elif weighted_score >= 60:
            recommendations.append("âš ï¸ **è‰¯å¥½**ï¼šç®€å†åŒ¹é…åº¦ä¸é”™ï¼Œä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´ã€‚")
        else:
            recommendations.append("âŒ **éœ€è¦ä¼˜åŒ–**ï¼šç®€å†ä¸èŒä½æè¿°åŒ¹é…åº¦è¾ƒä½ï¼Œå»ºè®®é‡ç‚¹ä¼˜åŒ–ã€‚")

        if missing_high:
            recommendations.append("\n### ğŸ”´ é«˜ä¼˜å…ˆçº§ç¼ºå¤±å…³é”®è¯ï¼ˆå¿…é¡»è¡¥å……ï¼‰")
            for s in missing_high[:10]:
                recommendations.append(f"- **{s['skill']}** (JDä¸­å‡ºç°{s['jd_mentions']}æ¬¡)")

        if missing_medium:
            recommendations.append("\n### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ç¼ºå¤±å…³é”®è¯ï¼ˆå»ºè®®è¡¥å……ï¼‰")
            for s in missing_medium[:10]:
                recommendations.append(f"- **{s['skill']}** (JDä¸­å‡ºç°{s['jd_mentions']}æ¬¡)")

        recommendations.append("\n### ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        recommendations.append("1. åœ¨ç®€å†ä¸­è¡¥å……ç¼ºå¤±çš„é«˜ä¼˜å…ˆçº§å…³é”®è¯")
        recommendations.append("2. ç¡®ä¿å…³é”®è¯å‡ºç°åœ¨ç®€å†çš„å¤šä¸ªéƒ¨åˆ†ï¼ˆæŠ€èƒ½ã€é¡¹ç›®ç»éªŒã€å·¥ä½œç»å†ï¼‰")
        recommendations.append("3. ä½¿ç”¨é‡åŒ–æŒ‡æ ‡çªå‡ºå·²åŒ¹é…çš„å…³é”®è¯")
        recommendations.append("4. é¿å…ä½¿ç”¨è¡¨æ ¼ã€å›¾ç‰‡ç­‰ ATS éš¾ä»¥è¯†åˆ«çš„æ ¼å¼")

        return "\n".join(recommendations)

    def _create_summary(self, match_result: dict, has_jd: bool) -> str:
        """Create human-readable summary"""
        if not has_jd:
            resume_kw_count = len(match_result.get('resume_keywords', []))
            return f"""# ğŸ“‹ ç®€å†å…³é”®è¯æå–ç»“æœ

âœ… æˆåŠŸæå– {resume_kw_count} ä¸ªå…³é”®è¯

ğŸ’¡ **æç¤º**: æä¾›èŒä½æè¿°ï¼ˆJDï¼‰å¯ä»¥è·å¾—ï¼š
- ATS åŒ¹é…åº¦åˆ†æ
- ç¼ºå¤±å…³é”®è¯è¯†åˆ«
- æ™ºèƒ½ä¼˜åŒ–å»ºè®®

è¯·åœ¨å‚æ•°ä¸­æ·»åŠ  `jd_text` æ¥è·å–å®Œæ•´çš„åŒ¹é…åˆ†æã€‚"""

        analysis = match_result['match_analysis']
        keywords = match_result['keywords']

        matched_high = keywords['matched']['high_priority']
        matched_medium = keywords['matched']['medium_priority']
        missing_high = keywords['missing']['high_priority']
        missing_medium = keywords['missing']['medium_priority']

        weighted_score = analysis['weighted_match_score']

        # Score emoji
        if weighted_score >= 80:
            score_emoji = "ğŸŸ¢"
        elif weighted_score >= 60:
            score_emoji = "ğŸŸ¡"
        else:
            score_emoji = "ğŸ”´"

        summary_lines = [
            "# ğŸ¯ ATS å…³é”®è¯åŒ¹é…åˆ†æ",
            "",
            f"## {score_emoji} åŒ¹é…åº¦: {weighted_score}%",
            f"- **åŠ æƒåŒ¹é…åº¦**: {weighted_score}% (åŸºäºå…³é”®è¯ä¼˜å…ˆçº§)",
            f"- **ç®€å•åŒ¹é…ç‡**: {analysis['simple_match_score']}% (å‚è€ƒ)",
            f"- **å·²åŒ¹é…**: {analysis['matched_count']} ä¸ªå…³é”®è¯",
            f"- **ç¼ºå¤±**: {analysis['missing_count']} ä¸ªå…³é”®è¯",
            ""
        ]

        if matched_high:
            summary_lines.append("### âœ… å·²åŒ¹é…å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")
            for kw in matched_high[:10]:
                summary_lines.append(f"- **{kw['skill']}** (ç®€å†ä¸­å‡ºç°{kw['mentions']}æ¬¡)")
            summary_lines.append("")

        if missing_high:
            summary_lines.append("### âŒ ç¼ºå¤±å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")
            for kw in missing_high[:10]:
                summary_lines.append(f"- **{kw['skill']}** (JDä¸­å‡ºç°{kw['jd_mentions']}æ¬¡)")
            summary_lines.append("")

        if missing_medium:
            summary_lines.append("### âš ï¸ ç¼ºå¤±å…³é”®è¯ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰")
            for kw in missing_medium[:5]:
                summary_lines.append(f"- **{kw['skill']}** (JDä¸­å‡ºç°{kw['jd_mentions']}æ¬¡)")
            summary_lines.append("")

        summary_lines.append("---")
        summary_lines.append("## ğŸ’¡ ä¼˜åŒ–å»ºè®®")
        summary_lines.append(match_result['recommendations'])

        return "\n".join(summary_lines)

    def _parse_resume_keywords_input(self, input_str: str) -> list[dict[str, Any]] | None:
        """
        Intelligently parse resume_keywords input from various formats.

        Supports:
        1. JSON array: [{"skill": "Python", "mentions": 3, ...}, ...]
        2. JSON object: {"keywords": [...], ...}
        3. Text summary from keyword_extraction tool (parse keywords from markdown)

        Args:
            input_str: Input string from user

        Returns:
            List of keyword dicts, or None if parsing fails
        """
        input_str = input_str.strip()

        # Try 1: Parse as JSON
        try:
            parsed = json.loads(input_str)

            if isinstance(parsed, list):
                # Direct array: [{"skill": "Python", ...}, ...]
                return parsed
            elif isinstance(parsed, dict) and 'keywords' in parsed:
                # Full result object: {"keywords": [...], ...}
                return parsed['keywords']
        except json.JSONDecodeError:
            pass

        # Try 2: Parse as text summary from keyword_extraction
        # Look for patterns like: "- **Python** (2 mentions) - explicit mention"
        keywords = []

        # Pattern 1: "- **Skill** (N mentions) - source"
        pattern1 = r'-\s+\*\*([^*]+)\*\*\s+\((\d+)\s+mentions?\)\s+-\s+(.+)'
        matches1 = re.findall(pattern1, input_str)
        for skill, mentions, source in matches1:
            keywords.append({
                "skill": skill.strip(),
                "mentions": int(mentions),
                "confidence": 1.0,
                "source": "parsed_from_text"
            })

        # Pattern 2: "- **Skill** - description"
        pattern2 = r'-\s+\*\*([^*]+)\*\*\s+-\s+(.+)'
        matches2 = re.findall(pattern2, input_str)
        for skill, description in matches2:
            # Skip if already matched by pattern1
            if not any(k['skill'] == skill.strip() for k in keywords):
                keywords.append({
                    "skill": skill.strip(),
                    "mentions": 1,
                    "confidence": 0.8,
                    "source": "parsed_from_text"
                })

        if keywords:
            return keywords

        # Parsing failed
        return None

    def _generate_standard_jd_requirements(self, position_name: str) -> str:
        """
        Use LLM to generate standard job requirements for a given position name.

        Args:
            position_name: Job position name (e.g., "ç®—æ³•å·¥ç¨‹å¸ˆå®ä¹ ", "å‰ç«¯å¼€å‘å·¥ç¨‹å¸ˆ")

        Returns:
            Generated job description text with standard requirements
        """
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ HR å’Œæ‹›è˜ä¸“å®¶ã€‚è¯·ä¸º"{position_name}"è¿™ä¸ªèŒä½ç”Ÿæˆæ ‡å‡†çš„æŠ€èƒ½è¦æ±‚æ¸…å•ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

# {position_name} - æ ‡å‡†èŒä½è¦æ±‚

## æ ¸å¿ƒæŠ€èƒ½è¦æ±‚ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
åˆ—å‡º 3-5 ä¸ªå¿…é¡»æŒæ¡çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œæ¯ä¸ªæŠ€èƒ½éœ€è¦åœ¨æè¿°ä¸­å‡ºç° 3 æ¬¡ä»¥ä¸Šã€‚

## é‡è¦æŠ€èƒ½è¦æ±‚ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰
åˆ—å‡º 5-8 ä¸ªå»ºè®®æŒæ¡çš„é‡è¦æŠ€èƒ½ï¼Œæ¯ä¸ªæŠ€èƒ½éœ€è¦åœ¨æè¿°ä¸­å‡ºç° 2 æ¬¡ã€‚

## åŠ åˆ†æŠ€èƒ½è¦æ±‚ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
åˆ—å‡º 3-5 ä¸ªåŠ åˆ†é¡¹æŠ€èƒ½ï¼Œæ¯ä¸ªæŠ€èƒ½å‡ºç° 1 æ¬¡å³å¯ã€‚

## èŒä½æè¿°
ç”¨ 2-3 æ®µè¯æè¿°è¿™ä¸ªèŒä½çš„å·¥ä½œå†…å®¹å’ŒèŒè´£ï¼Œè‡ªç„¶åœ°èå…¥ä¸Šè¿°æŠ€èƒ½å…³é”®è¯ã€‚

æ³¨æ„ï¼š
1. æŠ€èƒ½å…³é”®è¯è¦å…·ä½“ï¼ˆä¾‹å¦‚ï¼šPythonã€TensorFlowã€RAGï¼Œè€Œä¸æ˜¯"ç¼–ç¨‹èƒ½åŠ›"ã€"å­¦ä¹ èƒ½åŠ›"ï¼‰
2. æ ¹æ®èŒä½çº§åˆ«è°ƒæ•´è¦æ±‚ï¼ˆå®ä¹ ç”Ÿ vs é«˜çº§å·¥ç¨‹å¸ˆï¼‰
3. ç¡®ä¿å…³é”®è¯åœ¨æè¿°ä¸­è‡ªç„¶å‡ºç°æŒ‡å®šæ¬¡æ•°
4. ä½¿ç”¨ä¸­æ–‡è¾“å‡º

è¯·å¼€å§‹ç”Ÿæˆï¼š"""

        llm_config = {
            "provider": "deepseek",
            "model": "deepseek-chat",
            "mode": "chat",
            "completion_params": {
                "temperature": 0.7,
                "max_tokens": 2000
            }
        }

        # Retry logic for LLM invocation
        max_retries = 3
        retry_delay = 1  # Initial delay in seconds

        for attempt in range(max_retries):
            try:
                llm_result = self.session.model.llm.invoke(
                    model_config=LLMModelConfig(**llm_config),
                    prompt_messages=[UserPromptMessage(content=prompt)],
                    stream=False
                )

                response_text = llm_result.message.content.strip()

                # Check for empty response
                if not response_text:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ LLM returned empty JD (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                    else:
                        print(f"âŒ LLM returned empty JD after {max_retries} attempts, using fallback")
                        return f"""# {position_name} - æ ‡å‡†èŒä½è¦æ±‚

## æ ¸å¿ƒæŠ€èƒ½è¦æ±‚
æ ¹æ®èŒä½åç§°ï¼Œè¯·æä¾›å®Œæ•´çš„èŒä½æè¿°ä»¥è·å¾—æ›´å‡†ç¡®çš„åŒ¹é…åˆ†æã€‚

LLM ç”Ÿæˆå¤±è´¥: å¤šæ¬¡é‡è¯•åä»è¿”å›ç©ºå“åº”
"""

                return response_text

            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ LLM JD generation failed (attempt {attempt + 1}/{max_retries}): {str(e)}, retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                else:
                    print(f"âŒ LLM JD generation failed after {max_retries} attempts: {str(e)}")
                    return f"""# {position_name} - æ ‡å‡†èŒä½è¦æ±‚

## æ ¸å¿ƒæŠ€èƒ½è¦æ±‚
æ ¹æ®èŒä½åç§°ï¼Œè¯·æä¾›å®Œæ•´çš„èŒä½æè¿°ä»¥è·å¾—æ›´å‡†ç¡®çš„åŒ¹é…åˆ†æã€‚

LLM ç”Ÿæˆå¤±è´¥: {str(e)}
"""

        # Fallback
        return f"""# {position_name} - æ ‡å‡†èŒä½è¦æ±‚

## æ ¸å¿ƒæŠ€èƒ½è¦æ±‚
æ ¹æ®èŒä½åç§°ï¼Œè¯·æä¾›å®Œæ•´çš„èŒä½æè¿°ä»¥è·å¾—æ›´å‡†ç¡®çš„åŒ¹é…åˆ†æã€‚

LLM ç”Ÿæˆå¤±è´¥: æœªçŸ¥é”™è¯¯
"""

    def _extract_keywords_from_generated_jd(self, generated_jd: str) -> list[dict[str, Any]]:
        """
        Extract keywords from LLM-generated job description.
        Parse the structured output and create keyword list with priorities.

        Args:
            generated_jd: LLM-generated job description text

        Returns:
            List of keyword dictionaries with skill, mentions, priority, weight
        """
        keywords = []

        # Parse high-priority skills (mentioned 3+ times in the generated JD)
        high_priority_pattern = r"## æ ¸å¿ƒæŠ€èƒ½è¦æ±‚[^#]+"
        high_match = re.search(high_priority_pattern, generated_jd, re.DOTALL)
        if high_match:
            high_section = high_match.group(0)
            # Extract skill names (look for technical terms in Chinese/English)
            skills = re.findall(r'[A-Za-z][A-Za-z0-9+#\.]*(?:\.[A-Za-z]+)?', high_section)
            for skill in skills:
                if len(skill) > 1:  # Filter out single letters
                    keywords.append({
                        "skill": skill,
                        "mentions": 3,  # High priority = 3 mentions
                        "confidence": 1.0,
                        "source": "llm_generated",
                        "priority": "high",
                        "weight": 3.0
                    })

        # Parse medium-priority skills (mentioned 2 times)
        medium_priority_pattern = r"## é‡è¦æŠ€èƒ½è¦æ±‚[^#]+"
        medium_match = re.search(medium_priority_pattern, generated_jd, re.DOTALL)
        if medium_match:
            medium_section = medium_match.group(0)
            skills = re.findall(r'[A-Za-z][A-Za-z0-9+#\.]*(?:\.[A-Za-z]+)?', medium_section)
            for skill in skills:
                if len(skill) > 1 and skill not in [k['skill'] for k in keywords]:
                    keywords.append({
                        "skill": skill,
                        "mentions": 2,  # Medium priority = 2 mentions
                        "confidence": 1.0,
                        "source": "llm_generated",
                        "priority": "medium",
                        "weight": 2.0
                    })

        # Parse low-priority skills (mentioned 1 time)
        low_priority_pattern = r"## åŠ åˆ†æŠ€èƒ½è¦æ±‚[^#]+"
        low_match = re.search(low_priority_pattern, generated_jd, re.DOTALL)
        if low_match:
            low_section = low_match.group(0)
            skills = re.findall(r'[A-Za-z][A-Za-z0-9+#\.]*(?:\.[A-Za-z]+)?', low_section)
            for skill in skills:
                if len(skill) > 1 and skill not in [k['skill'] for k in keywords]:
                    keywords.append({
                        "skill": skill,
                        "mentions": 1,  # Low priority = 1 mention
                        "confidence": 1.0,
                        "source": "llm_generated",
                        "priority": "low",
                        "weight": 1.0
                    })

        return keywords

