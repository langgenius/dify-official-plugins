from typing import Any
from collections.abc import Generator
import time

from dify_plugin import Tool
from dify_plugin.entities.tool import ToolInvokeMessage
from dify_plugin.entities.model.llm import LLMModelConfig
from dify_plugin.entities.model.message import UserPromptMessage


class ResumeOptimizerTool(Tool):
    """
    Resume optimization tool with bilingual support and target position integration.

    This tool helps users optimize their resumes for specific job positions using LLM.
    It supports both file upload and text input, with bilingual prompts.
    """

    PROMPTS = {
        "zh_Hans": """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç®€å†ä¼˜åŒ–ä¸“å®¶ã€‚è¯·é’ˆå¯¹ã€{target_position}ã€‘å²—ä½ï¼Œç›´æ¥ç»™å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ã€‚

ç›®æ ‡å²—ä½ï¼š{target_position}

{detected_issues_section}

## é‡è¦çº¦æŸ

ç®€å†å†…å®¹å¯èƒ½æ˜¯ä» PDF/DOCX è½¬æ¢ä¸º Markdown çš„ï¼Œå¯èƒ½å­˜åœ¨æ ¼å¼è½¬æ¢é—®é¢˜ã€‚

**è¯·åªå…³æ³¨ç®€å†çš„å®è´¨å†…å®¹ä¼˜åŒ–**ï¼š
- å…³é”®è¯åŒ¹é…åº¦ï¼ˆæ˜¯å¦åŒ…å«å²—ä½è¦æ±‚çš„æ ¸å¿ƒæŠ€æœ¯æ ˆå’ŒæŠ€èƒ½ï¼‰
- å·¥ä½œç»å†å’Œé¡¹ç›®ç»éªŒçš„æè¿°ï¼ˆæ˜¯å¦çªå‡ºç›¸å…³ç»éªŒï¼‰
- æŠ€èƒ½å±•ç¤ºå’Œé‡åŒ–æˆæœï¼ˆæ˜¯å¦ç”¨æ•°æ®è¯´è¯ï¼‰
- å†…å®¹çš„ä¸“ä¸šæ€§å’Œé’ˆå¯¹æ€§ï¼ˆæ˜¯å¦ç¬¦åˆå²—ä½è¦æ±‚ï¼‰

**è¯·å¿½ç•¥ä»¥ä¸‹é—®é¢˜ï¼Œä¸è¦åœ¨ä¼˜åŒ–å»ºè®®ä¸­æåŠ**ï¼š
- Markdown æ ¼å¼é—®é¢˜ï¼ˆå¤šä½™ç©ºæ ¼ã€æ¢è¡Œã€ç¬¦å·ä¸¢å¤±ã€ç¼©è¿›ç­‰ï¼‰
- æ’ç‰ˆå’Œå¸ƒå±€é—®é¢˜
- æ–‡ä»¶æ ¼å¼é—®é¢˜

è¿™äº›æ ¼å¼é—®é¢˜å¯èƒ½æ˜¯è½¬æ¢å·¥å…·å¯¼è‡´çš„ï¼Œåœ¨åŸå§‹æ–‡ä»¶ä¸­ä¸å­˜åœ¨ã€‚ç”¨æˆ·ä¼šåœ¨åŸå§‹æ–‡ä»¶ä¸­åº”ç”¨ä½ çš„å†…å®¹ä¼˜åŒ–å»ºè®®ã€‚

## è¾“å‡ºè¦æ±‚

**ä¸è¦**è‡ªæˆ‘ä»‹ç»ã€ä¸è¦åˆ†æé—®é¢˜ã€ä¸è¦ä»‹ç»å·¥ä½œè®¡åˆ’ï¼Œ**ç›´æ¥å¼€å§‹è¾“å‡ºä¼˜åŒ–å»ºè®®**ã€‚

æŒ‰ç…§ç®€å†çš„å®é™…æ¨¡å—ç»“æ„ï¼ˆå¦‚ï¼šæ•™è‚²èƒŒæ™¯ã€å·¥ä½œç»å†ã€é¡¹ç›®ç»éªŒã€ä¸“ä¸šæŠ€èƒ½ç­‰ï¼‰ï¼Œé€ä¸€ç»™å‡ºä¼˜åŒ–å»ºè®®ã€‚

æ¯æ¡å»ºè®®å¿…é¡»åŒ…å«ï¼š
- **æ”¹å‰**ï¼šä»ç®€å†ä¸­æ‘˜å½•éœ€è¦ä¿®æ”¹çš„åŸæ–‡ï¼ˆä¿æŒåŸæ–‡æ ¼å¼ï¼‰
- **æ”¹å**ï¼šä¼˜åŒ–åçš„è¡¨è¿°ï¼ˆå¯ç›´æ¥å¤åˆ¶ç²˜è´´ä½¿ç”¨ï¼‰
- **ä¼˜åŒ–ç†ç”±**ï¼š1-2 å¥è¯è¯´æ˜ä¸ºä»€ä¹ˆè¿™æ ·æ”¹æ›´é€‚åˆã€{target_position}ã€‘å²—ä½

## è¾“å‡ºæ ¼å¼

### ğŸ“‹ [æ¨¡å—åç§°]

**æ”¹å‰**ï¼š
```
[ä»ç®€å†ä¸­æ‘˜å½•çš„åŸæ–‡]
```

**æ”¹å**ï¼š
```
[ä¼˜åŒ–åçš„è¡¨è¿°]
```

**ä¼˜åŒ–ç†ç”±**ï¼š[ç®€æ´è¯´æ˜]

---

### ğŸ“‹ [ä¸‹ä¸€ä¸ªæ¨¡å—åç§°]

**æ”¹å‰**ï¼š
```
[åŸæ–‡]
```

**æ”¹å**ï¼š
```
[ä¼˜åŒ–åçš„è¡¨è¿°]
```

**ä¼˜åŒ–ç†ç”±**ï¼š[ç®€æ´è¯´æ˜]

---

{issues_fix_section}

## ä¼˜åŒ–é‡ç‚¹

1. **å…³é”®è¯åŒ¹é…**ï¼šç¡®ä¿ç®€å†åŒ…å«ã€{target_position}ã€‘å²—ä½çš„æ ¸å¿ƒæŠ€æœ¯æ ˆå’Œå…³é”®è¯
2. **é‡åŒ–æˆæœ**ï¼šç”¨æ•°æ®è¯´è¯ï¼ˆå¦‚ï¼šæ€§èƒ½æå‡ X%ã€å¤„ç†é‡ X ä¸‡æ¬¡/æ—¥ï¼‰
3. **åŠ¨ä½œåŠ¨è¯**ï¼šä½¿ç”¨"è®¾è®¡ã€å®ç°ã€ä¼˜åŒ–ã€è´Ÿè´£"ç­‰å¼ºåŠ¨ä½œè¯ï¼Œé¿å…"å‚ä¸ã€äº†è§£"
4. **å²—ä½ç›¸å…³æ€§**ï¼šçªå‡ºä¸ç›®æ ‡å²—ä½æœ€ç›¸å…³çš„ç»éªŒï¼Œå¼±åŒ–æ— å…³å†…å®¹
5. **STAR æ³•åˆ™**ï¼šSituationï¼ˆèƒŒæ™¯ï¼‰â†’ Taskï¼ˆä»»åŠ¡ï¼‰â†’ Actionï¼ˆè¡ŒåŠ¨ï¼‰â†’ Resultï¼ˆç»“æœï¼‰

## æ³¨æ„äº‹é¡¹

- åªé’ˆå¯¹**éœ€è¦ä¼˜åŒ–çš„å†…å®¹**ç»™å‡ºå»ºè®®ï¼Œå·²ç»å¾ˆå¥½çš„éƒ¨åˆ†å¯ä»¥è·³è¿‡
- æ¯æ¡å»ºè®®éƒ½è¦**å…·ä½“ã€å¯æ“ä½œ**ï¼Œç”¨æˆ·å¯ä»¥ç›´æ¥å¤åˆ¶ç²˜è´´
- ä¿æŒç®€å†çš„**åŸæœ‰ç»“æ„å’Œé£æ ¼**ï¼Œä¸è¦å¤§å¹…æ”¹å˜æ’ç‰ˆ
- å¦‚æœç®€å†ä¸­æŸäº›æ¨¡å—ç¼ºå¤±ä½†å¯¹ç›®æ ‡å²—ä½é‡è¦ï¼Œå¯ä»¥å»ºè®®æ·»åŠ 

---

**ç°åœ¨å¼€å§‹è¾“å‡ºä¼˜åŒ–å»ºè®®**ï¼ˆä¸è¦ä»»ä½•å¼€åœºç™½ï¼Œç›´æ¥ä»ç¬¬ä¸€ä¸ªæ¨¡å—å¼€å§‹ï¼‰ï¼š

ç®€å†å†…å®¹ï¼š
{resume_content}""",

        "en_US": """You are a seasoned resume optimization expert. Please provide specific modification suggestions for the [{target_position}] position.

Target Position: {target_position}

{detected_issues_section}

## Important Constraints

The resume content may have been converted from PDF/DOCX to Markdown, which may introduce format conversion issues.

**Please focus ONLY on substantive content optimization**:
- Keyword matching (does it include core tech stack and skills required for the position)
- Work experience and project descriptions (does it highlight relevant experience)
- Skills showcase and quantified achievements (does it use data to demonstrate impact)
- Content professionalism and relevance (does it align with position requirements)

**Please IGNORE the following issues and do NOT mention them in your suggestions**:
- Markdown formatting issues (extra spaces, line breaks, missing symbols, indentation, etc.)
- Layout and formatting problems
- File format issues

These formatting issues may be caused by conversion tools and do not exist in the original file. Users will apply your content optimization suggestions to their original files.

## Output Requirements

**Do NOT** introduce yourself, analyze problems, or describe your work plan. **Start directly with optimization suggestions**.

Provide suggestions for each actual section in the resume (e.g., Education, Work Experience, Projects, Skills, etc.).

Each suggestion must include:
- **Before**: Original text from the resume (keep original format)
- **After**: Optimized version (ready to copy-paste)
- **Reason**: 1-2 sentences explaining why this change better fits the [{target_position}] position

## Output Format

### ğŸ“‹ [Section Name]

**Before**:
```
[Original text from resume]
```

**After**:
```
[Optimized version]
```

**Reason**: [Brief explanation]

---

### ğŸ“‹ [Next Section Name]

**Before**:
```
[Original text]
```

**After**:
```
[Optimized version]
```

**Reason**: [Brief explanation]

---

{issues_fix_section}

## Optimization Focus

1. **Keyword Matching**: Ensure resume includes core tech stack and keywords for [{target_position}]
2. **Quantified Achievements**: Use data (e.g., improved performance by X%, handled X requests/day)
3. **Action Verbs**: Use strong verbs like "designed, implemented, optimized, led" instead of "participated, familiar with"
4. **Job Relevance**: Highlight most relevant experience for target position, de-emphasize irrelevant content
5. **STAR Method**: Situation â†’ Task â†’ Action â†’ Result

## Guidelines

- Only provide suggestions for **content that needs improvement**; skip parts that are already good
- Each suggestion should be **specific and actionable**, ready to copy-paste
- Maintain the **original structure and style** of the resume, don't drastically change layout
- If important sections are missing for the target position, suggest adding them

---

**Start outputting optimization suggestions now** (no introduction, start directly from the first section):

Resume Content:
{resume_content}"""
    }

    def _invoke(self, tool_parameters: dict[str, Any]) -> Generator[ToolInvokeMessage]:
        """
        Invoke the resume optimizer tool.

        Args:
            tool_parameters: Tool parameters including resume_content, target_position, detected_issues, and language

        Returns:
            Generator of ToolInvokeMessage
        """
        try:
            # Extract and validate parameters
            target_position = tool_parameters.get('target_position', '').strip()
            detected_issues = tool_parameters.get('detected_issues', '').strip()
            language = tool_parameters.get('language', 'zh_Hans')

            # Get resume content from file upload or text input
            resume_content, error_msg = self._get_resume_content(tool_parameters, language)
            if error_msg:
                yield self.create_text_message(error_msg)
                return

            # Validate required parameters
            if not target_position:
                error_msg = "ç›®æ ‡å²—ä½ä¸èƒ½ä¸ºç©º" if language == 'zh_Hans' else "Target position cannot be empty"
                yield self.create_text_message(error_msg)
                return

            # Generate optimization suggestions using LLM
            result = self._optimize_resume_with_llm(resume_content, target_position, detected_issues, language)
            yield self.create_text_message(result)

        except Exception as e:
            error_msg = f"ä¼˜åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}" if language == 'zh_Hans' else f"Error during optimization: {str(e)}"
            yield self.create_text_message(error_msg)

    def _get_resume_content(self, tool_parameters: dict[str, Any], language: str) -> tuple[str, str]:
        """
        Extract resume content from text input.

        Returns:
            tuple: (resume_content, error_message)
        """
        # Get resume content from text input
        resume_content = tool_parameters.get('resume_content', '').strip()
        if not resume_content:
            error_msg = "è¯·è¾“å…¥ç®€å†å†…å®¹" if language == 'zh_Hans' else "Please input resume content"
            return "", error_msg

        return resume_content, ""

    def _optimize_resume_with_llm(self, resume_content: str, target_position: str, detected_issues: str, language: str) -> str:
        """Use LLM to generate resume optimization suggestions."""
        import json
        import traceback

        try:
            # Build detected issues section
            detected_issues_section = ""
            issues_fix_section = ""

            if detected_issues:
                if language == 'zh_Hans':
                    detected_issues_section = f"## å·²æ£€æµ‹åˆ°çš„é—®é¢˜\n\n{detected_issues}\n"
                    issues_fix_section = "\n5. **é—®é¢˜ä¿®å¤** - é’ˆå¯¹ä¸Šè¿°æ£€æµ‹åˆ°çš„é—®é¢˜æä¾›å…·ä½“ä¿®å¤å»ºè®®"
                else:
                    detected_issues_section = f"## Detected Issues\n\n{detected_issues}\n"
                    issues_fix_section = "\n5. **Issue Resolution** - Specific fixes for the detected issues above"

            # Build prompt using template
            prompt_template = self.PROMPTS.get(language, self.PROMPTS['zh_Hans'])
            prompt = prompt_template.format(
                target_position=target_position,
                resume_content=resume_content,
                detected_issues_section=detected_issues_section,
                issues_fix_section=issues_fix_section
            )

            # Prepare LLM request
            prompt_messages = [UserPromptMessage(content=prompt)]

            # Use system-configured LLM (user should configure DeepSeek in Dify settings)
            # This approach follows Dify's best practices for plugin LLM usage
            llm_config = {
                "provider": "deepseek",
                "model": "deepseek-chat",
                "mode": "chat",
                "completion_params": {
                    "temperature": 0.7,
                    "max_tokens": 2000
                }
            }

            # ğŸ” DEBUG: æ‰“å°é…ç½®ä¿¡æ¯
            print(f"ğŸ” DEBUG [resume_optimizer] LLM Config: {json.dumps(llm_config, indent=2, ensure_ascii=False)}")
            print(f"ğŸ” DEBUG [resume_optimizer] Prompt length: {len(prompt)} chars")
            print(f"ğŸ” DEBUG [resume_optimizer] Session type: {type(self.session)}")
            print(f"ğŸ” DEBUG [resume_optimizer] Session.model type: {type(self.session.model)}")
            print(f"ğŸ” DEBUG [resume_optimizer] Session.model.llm type: {type(self.session.model.llm)}")

            # Retry logic for LLM invocation
            max_retries = 3
            retry_delay = 1  # Initial delay in seconds

            for attempt in range(max_retries):
                try:
                    print(f"ğŸ” DEBUG [resume_optimizer] Attempt {attempt + 1}/{max_retries} - Calling LLM...")

                    # Invoke LLM
                    llm_result = self.session.model.llm.invoke(
                        model_config=LLMModelConfig(**llm_config),
                        prompt_messages=prompt_messages,
                        stream=False
                    )

                    # ğŸ” DEBUG: æ‰“å°åŸå§‹å“åº”ä¿¡æ¯
                    print(f"ğŸ” DEBUG [resume_optimizer] llm_result type: {type(llm_result)}")
                    print(f"ğŸ” DEBUG [resume_optimizer] llm_result: {llm_result}")
                    if hasattr(llm_result, '__dict__'):
                        print(f"ğŸ” DEBUG [resume_optimizer] llm_result.__dict__: {llm_result.__dict__}")

                    # Extract result
                    if llm_result and hasattr(llm_result, 'message') and hasattr(llm_result.message, 'content'):
                        response_text = llm_result.message.content.strip()
                        print(f"ğŸ” DEBUG [resume_optimizer] Response text length: {len(response_text)} chars")

                        # Check for empty response
                        if not response_text:
                            if attempt < max_retries - 1:
                                print(f"âš ï¸ LLM returned empty optimization suggestions (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...")
                                time.sleep(retry_delay)
                                retry_delay *= 2
                                continue
                            else:
                                print(f"âŒ LLM returned empty optimization suggestions after {max_retries} attempts")
                                return "LLMè°ƒç”¨è¿”å›ç©ºç»“æœï¼Œè¯·ç¨åé‡è¯•" if language == 'zh_Hans' else "LLM returned empty result, please retry later"

                        return response_text
                    else:
                        # No valid response - retry
                        print(f"âš ï¸ DEBUG [resume_optimizer] Invalid response structure")
                        if attempt < max_retries - 1:
                            print(f"âš ï¸ LLM returned invalid response (attempt {attempt + 1}/{max_retries}), retrying in {retry_delay}s...")
                            time.sleep(retry_delay)
                            retry_delay *= 2
                            continue
                        else:
                            return "LLMè°ƒç”¨è¿”å›ç©ºç»“æœ" if language == 'zh_Hans' else "LLM returned empty result"

                except Exception as e:
                    error_details = str(e)

                    # ğŸ” DEBUG: æ‰“å°å®Œæ•´å¼‚å¸¸ä¿¡æ¯
                    print(f"âŒ DEBUG [resume_optimizer] Exception type: {type(e).__name__}")
                    print(f"âŒ DEBUG [resume_optimizer] Exception args: {e.args}")
                    print(f"âŒ DEBUG [resume_optimizer] Full traceback:\n{traceback.format_exc()}")

                    # å°è¯•è·å–æ›´å¤šå¼‚å¸¸ä¿¡æ¯
                    if hasattr(e, 'response'):
                        print(f"âŒ DEBUG [resume_optimizer] e.response: {e.response}")
                    if hasattr(e, '__cause__'):
                        print(f"âŒ DEBUG [resume_optimizer] e.__cause__: {e.__cause__}")
                    if hasattr(e, '__context__'):
                        print(f"âŒ DEBUG [resume_optimizer] e.__context__: {e.__context__}")

                    # Check if it's a configuration error (don't retry)
                    if "Provider" in error_details and "does not exist" in error_details:
                        return f"è¯·åœ¨Difyè®¾ç½®ä¸­é…ç½®DeepSeekæä¾›å•†: {error_details}" if language == 'zh_Hans' else f"Please configure DeepSeek provider in Dify settings: {error_details}"

                    # For other errors, retry
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ LLM invocation failed (attempt {attempt + 1}/{max_retries}): {error_details}, retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                    else:
                        print(f"âŒ LLM invocation failed after {max_retries} attempts: {error_details}")
                        return f"LLMè°ƒç”¨å¤±è´¥: {error_details}" if language == 'zh_Hans' else f"LLM invocation failed: {error_details}"

            # Fallback (should not reach here)
            return "LLMè°ƒç”¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•" if language == 'zh_Hans' else "LLM invocation failed, please retry later"

        except Exception as e:
            error_details = str(e)
            print(f"âŒ DEBUG [resume_optimizer] Outer exception: {traceback.format_exc()}")
            return f"ä¼˜åŒ–è¿‡ç¨‹å‡ºé”™: {error_details}" if language == 'zh_Hans' else f"Optimization error: {error_details}"
